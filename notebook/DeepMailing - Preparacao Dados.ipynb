{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Mailing - Preparação de Dados\n",
    "\n",
    "O Objetivo do Deep Mailing é criar uma forma de se prever o CUP de um telefone baseado no seu historico. Para isso iremos criar um dataset que contem as features de cada um dos telefones que gerou CUP e criar um modelo para sua predição usando diversas técnicas.\n",
    "\n",
    "Esse notebook contém toda a logica de preparação do nosso dataset que será usado nos outros notebooks que implementam algoritmos de detecção de padrões (machine learning) especificos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuração do Kernel em Python\n",
    "\n",
    "A primeira tarefa no nosso Kernel é a importação dos módulos necessarios e a definição de diversos parametros operacionais como a localização de logs, os arquivos de dados a serem gerados e seus nomes além de configuração especificas de módulos.\n",
    "\n",
    "Os parametros do nosso Kernel que são definidos hardcoded são:\n",
    "* **log_location**: Caminho aonde serão gerados os logs desse kernel.\n",
    "* **arquivo_chamadas**: O caminho completo até o arquivo txt que contem os dados de chamadas\n",
    "* **arquivo_df_pickled**: O Arquivo pickle que será a serialização de um dataframe pandas com os dados brutos importados apartir do arquivo texto acima.\n",
    "* **arquivo_df_pickled_norm**: O Arquivo pickle que será a serialização de um dataframe pandas com os dados categorizados discretos explodidos em colunas discretas booleanas, ou seja: CLUSTER será explodido em NORM_CLUSTER_A01, NORM_CLUSTER_A02, NORM_CLUSTER_W01 e assim por diante, e essas colunas NORM_* tem apenas os valores possiveis de 0 ou 1.\n",
    "\n",
    "* **pd.options.display**: Configuracao do pandas para o numero maximo de colunas a ser mostrado quando renderizando o html de um dataframe.\n",
    "* **num_partitions**: Quando fazendo multiprocessamento dos dataframes, o numero de particoes em que o dataframe original sera repartido.\n",
    "* **num_cores**: O numero de cores que iremos assumir na maquina para o multiprocessamento de dataframes.\n",
    "* **Normalizado**: Variavel global que controla se os dados já estao normalizados, se sim, ou seja, detectamos um arquivo *arquivo_df_pickled_norm* no local indicado, então assumimos que não precisamos normalizar o dataframe\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dateutil.parser as parser\n",
    "import os.path\n",
    "import math\n",
    "import logging\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "from datetime import datetime\n",
    "\n",
    "log_location = \"../logs/\"\n",
    "arquivo_chamadas = \"../data/mailing_completo.txt.full\"\n",
    "arquivo_df_pickled = \"../intermediate/df.pickle\"\n",
    "arquivo_df_pickled_norm = \"../intermediate/df.norm.pickle\"\n",
    "\n",
    "pd.options.display.max_columns = 50\n",
    "num_partitions = 8\n",
    "num_cores = 8\n",
    "Normalizado = False\n",
    "\n",
    "#Delete Jupyter notebook root logger handler\n",
    "logger = logging.getLogger()\n",
    "logger.handlers = []\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(format=\"%(asctime)-15s %(message)s\",\n",
    "                    level=logging.DEBUG,\n",
    "                    filename=os.path.join(log_location,'prepare_data.log.' + \\\n",
    "                                          datetime.now().strftime(\"%Y%m%d%H%M%S.%f\") + '.log'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções de Conversão\n",
    "\n",
    "Essas funções são uteis para a conversão dos dados txt para os formats de dados corretos a serem usados nos dataframes do pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.debug(\"Declarando as funcoes globais\")\n",
    "\n",
    "def IsInt(s):\n",
    "    try: \n",
    "        int(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def IsIntAndGreaterZero(s):\n",
    "    return IsInt(s) and int(s)>0\n",
    "    \n",
    "def IsFloat(s):\n",
    "    try: \n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "    \n",
    "    \n",
    "def IsDatetime(s):\n",
    "    try: \n",
    "        parser.parse(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "    \n",
    "def ConverterInt(val):\n",
    "    val = val.replace(\",\",\".\")\n",
    "    if IsInt(val):\n",
    "        return int(val)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def ConverterFloat(val):\n",
    "    val = val.replace(\",\",\".\")\n",
    "    if IsFloat(val):\n",
    "        return float(val)\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "    \n",
    "def ConverterData(val):\n",
    "    if IsDatetime(val):\n",
    "        return parser.parse(val)\n",
    "    else:\n",
    "        return val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções para verificar existencia de arquivos\n",
    "\n",
    "As funções abaixo são usadas para verificar a existencia de arquivos diversos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IsCSVDataAvailable():\n",
    "    return os.path.isfile(arquivo_df_pickled)\n",
    "\n",
    "def IsNormDataAvailable():\n",
    "    return os.path.isfile(arquivo_df_pickled_norm)\n",
    "\n",
    "def IsTrainDataAvailable():\n",
    "    return os.path.isfile(arquivo_df_pickled_norm_train)\n",
    "    \n",
    "def IsNumpyArrayDataAvailable():\n",
    "    return os.path.isfile(arquivo_df_pickled_norm_train_x + \".npy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções para Paralelização via Multiprocessamento\n",
    "\n",
    "Como existem dataframes muito grandes, é necessario paralelizar o processamento de forma a utilizar toda a capacidade de processamento do computador. \n",
    "\n",
    "O Algoritmo é bastante simples, e se utiliza da criação de processos filhos para que se possa distribuir a carga de trabalho entre os seus filhos. Abaixo temos a funcao que permite essa serialização e distribuição para os processos filhos. Importante salientar que os dados são serializados e depois desserializados para a execução do processo filho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallelize_dataframe(func, data):\n",
    "    df = data['df']\n",
    "    df_split = np.array_split(df, num_partitions)\n",
    "    pool = Pool(num_cores)\n",
    "    items = list(data.items())\n",
    "    chunksize = len(data.items())\n",
    "    chunks = [items[i:i + chunksize ] for i in range(0, len(items), chunksize)]\n",
    "    df = pd.concat(pool.map(func, chunks))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções para Normalizar as Colunas do DataFrame\n",
    "\n",
    "Abaixo temos as funções que usaremos para desmembrar colunas que tem categorias em varias colunas com valores booleanos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_str(x):\n",
    "    return str(x)\n",
    "\n",
    "def func_strip(x):\n",
    "    return str(x).strip()\n",
    "\n",
    "def func_start_ALTA(x):\n",
    "    return str(x).startswith('ALTA')\n",
    "\n",
    "\n",
    "def func_to_execute_column_str(data):\n",
    "    data = dict(item for item in data)  # Convert back to a dict\n",
    "    logging.debug(\"Creating Binary Column:{} in {}\".format(data[\"col\"],  data[\"source_col\"]))\n",
    "    df = data['df']\n",
    "    df['NORM_' + data[\"source_col\"] + \"_\" + data[\"col\"]] = df.apply(lambda row: 1 if func_str(row[data[\"source_col\"]]) == data[\"col\"] else 0, axis=1)\n",
    "    return df\n",
    "    \n",
    "def CreateColumnStr(cols, df, source_col):\n",
    "    for col in cols:\n",
    "        df = parallelize_dataframe(func_to_execute_column_str, { \"df\" : df, \"source_col\" : source_col, \"col\" : col})\n",
    "    return df    \n",
    "\n",
    "def func_to_execute_column_strip(data):\n",
    "    data = dict(item for item in data)  # Convert back to a dict\n",
    "    logging.debug(\"Creating Binary Column:{} in {}\".format(data[\"col\"],  data[\"source_col\"]))\n",
    "    df = data['df']\n",
    "    df['NORM_' + data[\"source_col\"] + \"_\" + data[\"col\"]] = df.apply(lambda row: 1 if func_strip(row[data[\"source_col\"]]) == data[\"col\"] else 0, axis=1)\n",
    "    return df\n",
    "\n",
    "def CreateColumnStrip(cols, df, source_col):\n",
    "    for col in cols:\n",
    "        df = parallelize_dataframe(func_to_execute_column_strip, { \"df\" : df, \"source_col\" : source_col, \"col\" : col})\n",
    "    return df    \n",
    "\n",
    "def func_to_execute_column_ALTA(data):\n",
    "    data = dict(item for item in data)  # Convert back to a dict\n",
    "    logging.debug(\"Creating Binary Column:{} in {}\".format(data[\"col\"],  data[\"source_col\"]))\n",
    "    df = data['df']\n",
    "    df['NORM_' + data[\"source_col\"] + \"_\" + data[\"col\"]] = df.apply(lambda row: 1 if func_start_ALTA(row[data[\"source_col\"]]) == data[\"col\"] else 0, axis=1)\n",
    "    return df\n",
    "    \n",
    "def CreateColumnALTA(cols, df, source_col):\n",
    "    for col in cols:\n",
    "        df = parallelize_dataframe(func_to_execute_column_ALTA, { \"df\" : df, \"source_col\" : source_col, \"col\" : col})\n",
    "    return df    \n",
    "\n",
    "def func_to_execute_numeric(data):\n",
    "    data = dict(item for item in data)  # Convert back to a dict\n",
    "    logging.debug(\"Creating Numeric Column:{} \".format(data[\"source_col\"]))\n",
    "    df = data['df']\n",
    "    df['NORM_' + data[\"source_col\"]] = df.apply(lambda row: 1 if IsIntAndGreaterZero(row[data[\"source_col\"]]) else 0, axis=1)\n",
    "    return df\n",
    "\n",
    "def CreateLogColumn(df, source_col):\n",
    "    df = parallelize_dataframe(func_to_execute_numeric, { \"df\" : df, \"source_col\" : source_col})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tratamento dos dados de cada coluna no Dataframe\n",
    "\n",
    "Abaixo, temos a declaração de cada tipo de dado da coluna e a função que irá tratar a informação daquela coluna e colocar no formato correto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.debug(\"Declarando os tipos de dados no dataframe\")\n",
    "\n",
    "df_dtypes = {\n",
    "    \"CPF_CNPJ\": \"object\",\n",
    "    \"CARTEIRA\": \"object\",\n",
    "    \"SEGMENTO\": \"object\",\n",
    "    \"PRODUTO\": \"object\",\n",
    "    \"FILA\": \"object\",\n",
    "    \"STATUS_CONTRATO\": \"object\",\n",
    "    \"PROPENSAO\": \"object\",\n",
    "    \"ORIGEM\": \"object\",\n",
    "    \"DETALHE_ORIGEM\": \"object\",\n",
    "    \"STATUS_BUREAU\": \"object\",\n",
    "    \"STATUS_INTERNA\": \"object\",\n",
    "    \"DDD\": \"object\",\n",
    "    \"TELEFONE\": \"object\",\n",
    "    \"TELRUIM_RENITENCIA\": \"object\",\n",
    "    \"TELRUIM_DISCADOR\": \"object\",\n",
    "    \"STATUS_TELEFONE\": \"object\",\n",
    "    \"OPERADORA\": \"object\",\n",
    "    \"ORIGEM_ULTIMA_ATUALIZACAO\": \"object\",\n",
    "    \"PRIMEIRA_ORIGEM\": \"object\"\n",
    "}\n",
    "\n",
    "converters = {\n",
    "    \"ATRASO\":  ConverterInt,\n",
    "    \"VALOR\": ConverterFloat,\n",
    "    \"DT_ENTRADA\" : ConverterData,\n",
    "    \"NLOC\": ConverterInt,\n",
    "    \"SCORE_C\": ConverterInt,\n",
    "    \"SCORE_E\": ConverterInt,\n",
    "    \"RENDA\": ConverterFloat,\n",
    "    \"DT_DEVOLUCAO\": ConverterData,\n",
    "    \"VLRISCO\": ConverterFloat,\n",
    "    \"SCORE_ZANC_C\": ConverterInt,\n",
    "    \"SCORE_ZANC_E\": ConverterInt,\n",
    "    \"SCORE_ZANC\": ConverterInt,\n",
    "    \"DATA_PRIMEIRA_ORIGEM\": ConverterData,\n",
    "    \"DATA_ULTIMA_ATUALIZACAO\": ConverterData,\n",
    "    \"TENTATIVAS\": ConverterInt,\n",
    "    \"ULT_ARQ_BUREAU\": ConverterData,\n",
    "    \"DATA_MAILING\": ConverterData,\n",
    "    \"LIGACOES\": ConverterInt,\n",
    "    \"CUP\": ConverterInt\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga dos dados CSV\n",
    "\n",
    "Abaixo, carregamos os dados em CSV em um dataframe pandas e apagamos as colunas desnecessárias para o nosso uso, e gravamos o dataframe em um arquivo para reuso pelos algoritmos.\n",
    "\n",
    "Caso já exista o arquivo de dataframe, lemos o arquivo e criamos o dataframe para normalização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpar_df(chamadas):\n",
    "    del chamadas[\"CPF_CNPJ\"]\n",
    "    del chamadas[\"PRODUTO\"]\n",
    "    del chamadas[\"FILA\"]\n",
    "    del chamadas[\"STATUS_CONTRATO\"]\n",
    "    del chamadas[\"DETALHE_ORIGEM\"]\n",
    "    del chamadas[\"TELEFONE\"]\n",
    "    del chamadas[\"TELRUIM_RENITENCIA\"]\n",
    "    del chamadas[\"TELRUIM_DISCADOR\"]\n",
    "    del chamadas[\"OPERADORA\"]\n",
    "    del chamadas[\"ORIGEM_ULTIMA_ATUALIZACAO\"]\n",
    "    del chamadas[\"PRIMEIRA_ORIGEM\"]\n",
    "    del chamadas[\"ATRASO\"] \n",
    "    del chamadas[\"VALOR\"]\n",
    "    del chamadas[\"DT_ENTRADA\" ]\n",
    "    del chamadas[\"NLOC\"]\n",
    "    del chamadas[\"SCORE_C\"]\n",
    "    del chamadas[\"SCORE_E\"]\n",
    "    del chamadas[\"RENDA\"]\n",
    "    del chamadas[\"DT_DEVOLUCAO\"]\n",
    "    del chamadas[\"VLRISCO\"]\n",
    "    del chamadas[\"SCORE_ZANC_C\"]\n",
    "    del chamadas[\"SCORE_ZANC_E\"]\n",
    "    del chamadas[\"SCORE_ZANC\"]\n",
    "    del chamadas[\"DATA_PRIMEIRA_ORIGEM\"]\n",
    "    del chamadas[\"DATA_ULTIMA_ATUALIZACAO\"]\n",
    "    del chamadas[\"ULT_ARQ_BUREAU\"]\n",
    "    return chamadas\n",
    "\n",
    "logging.debug(\"Carregando dos dados em CSV ou normalizados, dependendo da existencia deles...\")\n",
    "if not IsCSVDataAvailable():\n",
    "    chamadas = pd.read_csv(arquivo_chamadas, sep=\"|\", dtype=df_dtypes, converters = converters)\n",
    "    logging.debug(\"CSV carregado, limpando colunas desnecessarias\")\n",
    "    chamadas = limpar_df(chamadas)\n",
    "    logging.debug(\"Gravando DataFrame Pandas gerado...\")\n",
    "    chamadas.to_pickle(arquivo_df_pickled)\n",
    "else:\n",
    "    if not IsNormDataAvailable():    \n",
    "        chamadas = pd.read_pickle(arquivo_df_pickled)\n",
    "    else:\n",
    "        chamadas = pd.read_pickle(arquivo_df_pickled_norm)\n",
    "        Normalizado = True\n",
    "        \n",
    "logging.debug(\"Normalizado:{}\".format(Normalizado))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculo das datas maximas e Minimas\n",
    "\n",
    "Precisamos então calcular as datas minimas e maximas no nosso mailing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.debug(\"Calculando Datas minimas e maximas...\")\n",
    "data_maxima_mailing = chamadas.DATA_MAILING.max()\n",
    "data_minima_mailing = chamadas.DATA_MAILING.min()\n",
    "\n",
    "print(\"Max:{} Min:{}\".format(data_maxima_mailing, data_minima_mailing))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizamos a Coluna de Carteiras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.debug(\"Normalizando Carteiras...\")\n",
    "if not Normalizado:\n",
    "    Carteiras = set([x for x in chamadas.CARTEIRA.unique()[:-1] if len(x) == 3])\n",
    "    chamadas = CreateColumnStr(Carteiras,chamadas, 'CARTEIRA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizamos a Coluna de Segmentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.debug(\"Normalizando Segmentos...\")\n",
    "if not Normalizado:\n",
    "    Segmentos = set([x.strip() for x in chamadas.SEGMENTO.unique()[:-1] if len(x.strip()) == 2])\n",
    "    chamadas = CreateColumnStrip(Segmentos,chamadas, 'SEGMENTO')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizamos a Coluna de Chamadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.debug(\"Normalizando Chamadas...\")\n",
    "if not Normalizado:\n",
    "    Propensao = set([x[:4] for x in chamadas.PROPENSAO.unique() if str(x).startswith(\"ALTA\")])\n",
    "    chamadas = CreateColumnALTA(Propensao,chamadas, 'PROPENSAO')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizamos a Coluna de Origem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.debug(\"Normalizando Origem...\")\n",
    "if not Normalizado:\n",
    "    Origem = set([x for x in chamadas.ORIGEM.unique()[:-1]])\n",
    "    chamadas = CreateColumnStr(Origem,chamadas, 'ORIGEM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizamos a Coluna de Status de Bureau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.debug(\"Normalizando StatusBureau...\")\n",
    "if not Normalizado:\n",
    "    StatusBureau = set([str(x) for x in chamadas.STATUS_BUREAU.unique()])\n",
    "    chamadas = CreateColumnStr(StatusBureau,chamadas, 'STATUS_BUREAU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizamos a Coluna de Status Interna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.debug(\"Normalizando StatusInterna...\")\n",
    "if not Normalizado:\n",
    "    StatusInterna = set([str(x) for x in chamadas.STATUS_INTERNA.unique()[:-1]])\n",
    "    chamadas = CreateColumnStr(StatusInterna,chamadas, 'STATUS_INTERNA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizamos a Coluna de Status de Telefone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.debug(\"Normalizando Telefone...\")\n",
    "if not Normalizado:\n",
    "    StatusTelefone = set([str(x) for x in chamadas.STATUS_TELEFONE.unique()[:-1]])\n",
    "    chamadas = CreateColumnStr(StatusTelefone,chamadas, 'STATUS_TELEFONE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizamos a Coluna de DDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.debug(\"Normalizando DDD...\")\n",
    "if not Normalizado:\n",
    "    DDD = set([str(x) for x in chamadas.DDD.unique()[:-1]])\n",
    "    chamadas = CreateColumnStr(DDD,chamadas, 'DDD')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizamos as Colunas com as quantidades de tentativas, ligacoes e CUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.debug(\"Normalizando LIGACOES E TAL...\")\n",
    "if not Normalizado:\n",
    "    chamadas = CreateLogColumn(chamadas,'TENTATIVAS')\n",
    "    chamadas = CreateLogColumn(chamadas,'LIGACOES')\n",
    "    chamadas = CreateLogColumn(chamadas,'CUP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finalmente removemos as colunas que normalizamos e salvamos o dataframe..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.debug(\"Removendo Campos desnecessarios e pickling...\")\n",
    "if not Normalizado:\n",
    "    del chamadas['NUMERO']\n",
    "    del chamadas['TENTATIVAS']\n",
    "    del chamadas['LIGACOES']\n",
    "    del chamadas['CUP']\n",
    "    del chamadas['DDD']\n",
    "    del chamadas['STATUS_TELEFONE']\n",
    "    del chamadas['STATUS_INTERNA']\n",
    "    del chamadas['STATUS_BUREAU']\n",
    "    del chamadas['ORIGEM']\n",
    "    del chamadas['SEGMENTO']\n",
    "    del chamadas['PROPENSAO']\n",
    "    del chamadas['CARTEIRA']\n",
    "    chamadas.to_pickle(arquivo_df_pickled_norm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
